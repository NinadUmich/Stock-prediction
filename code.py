# -*- coding: utf-8 -*-
"""TermProj

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fdDsFqcpxcCPAVYD3M9fwrTM4TyD2hE7
"""

# !pip install yfinance pandas numpy scikit-learn matplotlib seaborn tensorflow torch

"""Install & Import libs, download stock data"""

import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import MinMaxScaler
import torch
import torch.nn as nn
import torch.optim as optim
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping

# Choose your stock ticker
ticker = "AAPL"   # AAPL, TSLA, MSFT, NVDA
data = yf.download(ticker, start="2015-01-01", end="2025-01-01")

# Flatten column names
data.columns = data.columns.get_level_values(1)
data.columns = ['Close', 'High', 'Low', 'Open', 'Volume']
print(data.head())

"""Preprocess with OHLCV + MACD; Sequence Creation






"""

# Use OHLCV
features = data[['Open', 'High', 'Low', 'Close', 'Volume']].copy()

# Add MACD + Signal
ema12 = features['Close'].ewm(span=12, adjust=False).mean()
ema26 = features['Close'].ewm(span=26, adjust=False).mean()
features['MACD'] = ema12 - ema26
features['MACD_Signal'] = features['MACD'].ewm(span=9, adjust=False).mean()

# Fill NaNs
features = features.fillna(0)

# Scale all features
scaler = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler.fit_transform(features)

num_features = scaled_data.shape[1]  # should be 7
print("Scaled data shape:", scaled_data.shape)

def create_sequences_multi(data, seq_length, horizon, target_col=3):
    X, y = [], []
    for i in range(len(data) - seq_length - horizon):
        X.append(data[i:i+seq_length])  # (seq_length, num_features)
        y.append(data[i+seq_length:i+seq_length+horizon, target_col])  # predict Close
    return np.array(X), np.array(y)

seq_length = 60
horizon = 30
X_multi, y_multi = create_sequences_multi(scaled_data, seq_length, horizon)

print("X_multi:", X_multi.shape, "y_multi:", y_multi.shape)

"""LSTM model training"""

early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)

model_lstm_30 = Sequential([
    LSTM(64, return_sequences=True, input_shape=(seq_length, num_features)),
    Dropout(0.2),
    LSTM(64),
    Dropout(0.2),
    Dense(horizon)
])

model_lstm_30.compile(optimizer='adam', loss='mean_squared_error')

history = model_lstm_30.fit(
    X_multi, y_multi,
    epochs=100,
    batch_size=32,
    verbose=1,
    validation_split=0.1,
    callbacks=[early_stop]
)

"""Training vs Validation Loss Plot (LSTM)"""

plt.figure(figsize=(8,5))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.title(f'{horizon}-Day LSTM Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

"""Plot Actual vs Predicted (LSTM)"""

# Predict
pred_multi_scaled = model_lstm_30.predict(X_multi)

# Separate scaler for Close only
scaler_close = MinMaxScaler(feature_range=(0,1))
scaled_close = scaler_close.fit_transform(data[['Close']].values)

# Use scaler_close.inverse_transform for predictions
pred_multi_price = np.vstack([
    scaler_close.inverse_transform(pred_multi_scaled[:, i].reshape(-1, 1)).ravel()
    for i in range(horizon)
]).T

y_multi_price = np.vstack([
    scaler_close.inverse_transform(y_multi[:, i].reshape(-1, 1)).ravel()
    for i in range(horizon)
]).T

# Last forecast window
last_idx = -1
true_last = y_multi_price[last_idx]
pred_last = pred_multi_price[last_idx]

plt.figure(figsize=(10,5))
plt.plot(true_last, label=f'Actual (next {horizon} days)', color='blue')
plt.plot(pred_last, label='LSTM Forecast', color='orange')
plt.title(f'{ticker} {horizon}-Day Forecast from Last Window (LSTM)')
plt.xlabel('Days Ahead')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()

"""RMSE and MAE for LSTM"""

from sklearn.metrics import mean_squared_error, mean_absolute_error
import numpy as np

def evaluate_forecast(y_true, y_pred):
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    mae = mean_absolute_error(y_true, y_pred)
    return rmse, mae

rmse_lstm, mae_lstm = evaluate_forecast(y_multi_price.flatten(),
                                        pred_multi_price.flatten())
print(f"LSTM RMSE: {rmse_lstm:.4f}, MAE: {mae_lstm:.4f}")

"""LSTM is done. Now move on to transformer model with attention."""

# Positional Encoding
class PositionalEncoding(nn.Module):
    def __init__(self, d_model, max_len=5000):
        super().__init__()
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                             (-torch.log(torch.tensor(10000.0)) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)
        self.register_buffer('pe', pe)

    def forward(self, x):
        return x + self.pe[:, :x.size(1), :]

# Transformer Model
class TransformerTimeSeries(nn.Module):
    def __init__(self, feature_size=7, num_layers=2, num_heads=8,
                 hidden_dim=128, dropout=0.1, horizon=15):
        super().__init__()
        self.input_layer = nn.Linear(feature_size, hidden_dim)
        self.pos_encoder = PositionalEncoding(hidden_dim)
        encoder_layers = nn.TransformerEncoderLayer(
            d_model=hidden_dim,
            nhead=num_heads,
            dropout=dropout,
            batch_first=True
        )
        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers=num_layers)
        self.fc_out = nn.Linear(hidden_dim, horizon)

    def forward(self, src):
        src = self.input_layer(src)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = self.fc_out(output[:, -1, :])  # last time step
        return output

"""Prepare data for PyTorch"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

X_torch = torch.tensor(X_multi, dtype=torch.float32).to(device)
y_torch = torch.tensor(y_multi, dtype=torch.float32).to(device)

# During preprocessing
scaler_all = MinMaxScaler(feature_range=(0, 1))
scaled_data = scaler_all.fit_transform(features)

scaler_close = MinMaxScaler(feature_range=(0, 1))
scaled_close = scaler_close.fit_transform(data[['Close']].values)

"""Split into training and validation sets"""

# Split into training and validation sets (90/10 split)
split_idx = int(0.9 * len(X_torch))
X_train, y_train = X_torch[:split_idx], y_torch[:split_idx]
X_val, y_val = X_torch[split_idx:], y_torch[split_idx:]

train_losses, val_losses = [], []

"""Train Transformer, with attention"""

# Initialize model and training setup
model_trans = TransformerTimeSeries(feature_size=num_features, horizon=horizon).to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model_trans.parameters(), lr=0.001)

epochs = 100
patience = 10
min_epochs = 10
best_loss = np.inf
patience_counter = 0

for epoch in range(epochs):
    # --- Training ---
    model_trans.train()
    optimizer.zero_grad()
    output = model_trans(X_train)
    train_loss = criterion(output, y_train)
    train_loss.backward()
    optimizer.step()

    # --- Validation ---
    model_trans.eval()
    with torch.no_grad():
        val_output = model_trans(X_val)
        val_loss = criterion(val_output, y_val)

    # --- Logging ---
    train_losses.append(train_loss.item())
    val_losses.append(val_loss.item())

    if val_loss.item() < best_loss - 1e-5:
        best_loss = val_loss.item()
        patience_counter = 0
        best_model_state = model_trans.state_dict()
    else:
        if epoch >= min_epochs:
            patience_counter += 1

    if (epoch + 1) % 5 == 0:
        print(f"Epoch {epoch+1}/{epochs}, Train: {train_loss.item():.6f}, Val: {val_loss.item():.6f}, Best Val: {best_loss:.6f}")

    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch+1}")
        break

# Load best model
model_trans.load_state_dict(best_model_state)

"""Plot Training vs Validation Loss (Transformers)"""

plt.figure(figsize=(8,5))
plt.plot(train_losses, label='Training Loss')
plt.plot(val_losses, label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss (MSE)')
plt.title(f'{horizon}-day Transformer Training vs Validation Loss')
plt.legend()
plt.grid(True)
plt.show()

"""Plot the Actual vs. Predicted Plot (Transformers)"""

model_trans.eval()
with torch.no_grad():
    preds_trans_scaled = model_trans(X_torch).cpu().numpy()

# Inverse transform predictions and ground truth using scaler_close
preds_trans_price = np.vstack([
    scaler_close.inverse_transform(preds_trans_scaled[:, i].reshape(-1, 1)).ravel()
    for i in range(horizon)
]).T

y_trans_price = np.vstack([
    scaler_close.inverse_transform(y_multi[:, i].reshape(-1, 1)).ravel()
    for i in range(horizon)
]).T


# Last forecast window
last_idx = -1
true_last = y_trans_price[last_idx]
pred_last = preds_trans_price[last_idx]

plt.figure(figsize=(10,5))
plt.plot(true_last, label=f'Actual (next {horizon} days)', color='blue')
plt.plot(pred_last, label='Transformer Forecast', color='green')
plt.title(f'{ticker} {horizon}-Day Forecast from Last Window (Transformer)')
plt.xlabel('Days Ahead')
plt.ylabel('Price')
plt.legend()
plt.grid(True)
plt.show()

"""RMSE and MAE for Transformer"""

# For Transformer
rmse_trans, mae_trans = evaluate_forecast(y_trans_price.flatten(),
                                          preds_trans_price.flatten())
print(f"Transformer RMSE: {rmse_trans:.4f}, MAE: {mae_trans:.4f}")