What I Built

- Implemented LSTM and Transformer models in TensorFlow and PyTorch for sequential prediction tasks.
- Designed attention-based sequence encodings to capture long-range temporal dependencies.
- Optimized training workflows using GPU acceleration, enabling faster convergence and scalable experimentation.

How I Evaluated It

- Benchmarked model performance across four datasets.
- Measured forecasting accuracy using RMSE and MAE.
- Analyzed convergence and prediction stability using loss curves and actual vs. predicted trajectory plots.

Why It Matters

- The modeling and validation techniques used in this project are directly applicable to robotics and autonomous systems, including trajectory prediction, planning, and sensor fusion pipelines that rely on reliable temporal forecasting.
